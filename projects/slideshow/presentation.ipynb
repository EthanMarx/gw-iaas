{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22a8f06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enabling Deep Learning in Gravitational Wave Physics With Inference-as-a-Service\n",
    "## Alec Gunny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20874eee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep Learning in Gravitational Wave Astronomy\n",
    "<img src=\"images/gw-dl-use-cases.png\" height=\"auto\" width=\"980px\" style=\"top:10px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6847f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "- [Introduction](#Introduction)\n",
    "- A naive model\n",
    "- Inference-as-a-service concepts\n",
    "- Results on LIGO data\n",
    "- Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580046b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## Deep learning inference - concepts and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab910a37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning Inference\n",
    "- Model has been trained - how do we use it to do the task it was trained for?\n",
    "    - i.e. make **inferences** about the likelihood of some quantity conditioned on new data\n",
    "- What are the demands of the environment in which the task needs to be performed?\n",
    "    - Do inferences need to be returned quickly once the data becomes available?\n",
    "        - Low-**latency** or **online** inference\n",
    "        - E.g. event detection for MMA triggers\n",
    "    - Do we need to perform a lot of inferences and only care how long it takes them _all_ to finish?\n",
    "        - High-**throughput** or **offline** inference\n",
    "        - E.g. searches through archival data or model validation\n",
    "    - What are our constraints in the total **expense** we can afford to incur for processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069e7e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning Inference - Requirements\n",
    "- The set of operations that constitute the model\n",
    "- The optimized (trained) set of model parameters or \"weights\"\n",
    "- Hardware capable of meeting latency/throughput/expense requirements\n",
    "    - CPUs, GPUs, TPUs, FPGA\n",
    "- Data\n",
    "- Pipeline which feeds data into the model and does something with the corresponding outputs\n",
    "    - Model is just a _component_ of this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cebc08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning Inference - Challenges\n",
    "<div class=\"float200 padded\">\n",
    "    <img src=\"images/hardware-logos.png\" width=\"300px\" height=\"auto\" class=\"right unpadded\"/>\n",
    "    <p class=\"left\">Access to and familiarity with accelerated hardware</p>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/framework-logos.png\" width=\"300px\" height=\"auto\" class=\"left padded\" />\n",
    "    <p class=\"right\"> Managing, leveraging, and translating across deep learning software stacks\n",
    "</div>\n",
    "\n",
    "<div></div>\n",
    "<div></div>\n",
    "<div class=\"float100\">\n",
    "    <img src=\"images/distributing-nns.png\" width=\"250px\" height=\"auto\" class=\"right unpadded\"/>\n",
    "    <p class=\"left\">Distributing updated models to dependent users and applications</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14159eb9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Not just inference problems\n",
    "- Large-scale offline validation required to quantify advantages of new research and automate model re-training\n",
    "    - Faster processing times mean faster iteration on novel ideas\n",
    "- Deployment on \"real\", uncontrolled data critical to:\n",
    "    - identifying failure modes and improving understanding of model behavior\n",
    "    - evaluating the correlation between validation metrics and true success criterion\n",
    "        - Identify and remove signal leakage in training pipelines\n",
    "        - Align training and test settings - can our algorithm optimize our latency/throughput/expense cost function better than its alternative?\n",
    "- O4 run will collect !X! amount of data per day\n",
    "    - Answering these questions now will ensure that we have the right combination of systems/algorithms in place to extract as much physics as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd54a08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The traditional deep learning inference model\n",
    "### Building an example pipeline using PyTorch\n",
    "- Illustrate the challenges outlined above\n",
    "- Motivate the requirements of an improved paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bd3b1",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Begin with a few imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fabf2f3",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Empty, Queue\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# some functionality will be necessarily\n",
    "# buried in here. If you're interested,\n",
    "# feel free to take a look\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92234296",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define our model: a simple multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea2ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for size in hidden_sizes:\n",
    "            self.layers.append(torch.nn.Linear(input_size, size))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            input_size = size\n",
    "\n",
    "        self.layers.append(torch.nn.Linear(input_size, 1))\n",
    "        self.layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0fedb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "During training:\n",
    "- instantiate an instance of this model then optimize its parameters\n",
    "- export these optimized parameters for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 64\n",
    "HIDDEN_SIZES = [256, 128, 64]\n",
    "model = MLP(INPUT_SIZE, HIDDEN_SIZES)\n",
    "\n",
    "# typically do some training here\n",
    "# for i in range(num_epochs):\n",
    "#    for x in dataset:\n",
    "#        do_a_gradient_step(model, x)\n",
    "# now our model has optimized parameters\n",
    "\n",
    "# export these parameter values somewhere\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d06e6f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At inference time, load in these optimized model weights and use them to map inputs to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858cd8bc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5128155], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the operations that constitute a model\n",
    "inference_model = MLP(INPUT_SIZE, HIDDEN_SIZES).cuda(0)\n",
    "\n",
    "# set the values of the parameters of these\n",
    "# operations to their optimized values\n",
    "inference_model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "# create an array on the CPU\n",
    "x = np.random.randn(INPUT_SIZE).astype(\"float32\")\n",
    "with torch.no_grad():\n",
    "    # move it on to the GPU\n",
    "    x = torch.from_numpy(x).cuda(0)\n",
    "\n",
    "    # use the model infer predictions on the GPU\n",
    "    y = inference_model(x)\n",
    "\n",
    "# move these predictions back to the CPU\n",
    "# for downstream processing\n",
    "y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c846d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inference on a dataset\n",
    "- Generally interested in using the model for many thousands or millions of inferences\n",
    "- Start with the simplest case: a dataset that can fit into memory at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b55ac3d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 198 ms, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "N = 5 * 10**5  # number of observations in our dataset\n",
    "dataset = np.random.randn(N, 64).astype(\"float32\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def do_some_inference(model, dataset, batch_size=8, device_index=0):\n",
    "    # move the data to the GPU in bulk\n",
    "    gpu_dataset = torch.from_numpy(dataset).cuda(device_index)\n",
    "\n",
    "    # iterate through it in batches and yield predictions\n",
    "    dataset = torch.utils.data.TensorDataset(gpu_dataset)\n",
    "    for [x] in torch.utils.data.DataLoader(dataset, batch_size=batch_size):\n",
    "        y = model(x)\n",
    "        yield y.cpu().numpy()\n",
    "\n",
    "# run through the dataset and get a rough time estimate\n",
    "%time outputs = [y for y in do_some_inference(inference_model, dataset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c72d79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Seems to work pretty fast, but how well are we utilizing the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871dcaac",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f26ca1fb5d4118888b1517e944e3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with utils.GpuUtilProgress(gpu_ids=0) as progbar:\n",
    "    task_id = progbar.add_task(\"[cyan]Inference\", total=N)\n",
    "\n",
    "    outputs = []\n",
    "    for y in do_some_inference(model, dataset):\n",
    "        outputs.append(y)\n",
    "        progbar.update(task_id, advance=len(y))\n",
    "\n",
    "output = np.concatenate(outputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc980d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yikes, only around 20%!\n",
    "- GPUs are expensive, can we improve things via parallel execution (assuming we can't change the batch size)?\n",
    "- First attempt: Naive (and sloppy) implementation using threading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c22113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78903b547544432d8d112b6e7269decc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = Queue()\n",
    "\n",
    "def task(dataset_chunk, device_index):\n",
    "    # for each inference task, create a copy of the\n",
    "    # model on the indicated GPU device\n",
    "    model = MLP(INPUT_SIZE, HIDDEN_SIZES).cuda(device_index)\n",
    "    model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "    # iterate through our dataset and send\n",
    "    # the results back to the main thread\n",
    "    for y in do_some_inference(\n",
    "        model, dataset_chunk, device_index=device_index\n",
    "    ):\n",
    "        q.put(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631ecde",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run this task on multiple parallel threads (if we tried to use processes, Torch would complain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5a735",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_jobs = 4\n",
    "\n",
    "with utils.GpuUtilProgress(0) as progbar:\n",
    "    task_id = progbar.add_task(f\"Inference with {num_jobs} jobs\", total=N)\n",
    "\n",
    "    # create a pool of threads to do inference in parallel\n",
    "    with ThreadPoolExecutor(4) as pool:\n",
    "        # split the dataset into chunks and submit\n",
    "        # inference on them as tasks to the pool\n",
    "        [pool.submit(task, x, 0) for x in np.array_split(dataset, num_jobs)]\n",
    "\n",
    "        # iterate through the dataset\n",
    "        outputs = []\n",
    "        while not progbar.finished:\n",
    "            outputs.extend(q.get())\n",
    "            progbar.update(task_id, completed=len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2946d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It looks like things actually got worse!\n",
    "- Extracting good GPU performance is rarely simple or intuitive\n",
    "- What next?\n",
    "\n",
    "After spending a few hours perusing the PyTorch documentation and experimenting, we come up with the following basic functional implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200d0109",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def parallel_inference(X, num_gpus, jobs_per_gpu, progbar):\n",
    "    num_jobs = num_gpus * jobs_per_gpu\n",
    "    task_id = progbar.add_task(\n",
    "        f\"[cyan]{num_gpus} GPUs/{num_jobs} jobs\",\n",
    "        total=len(X),\n",
    "        start=False\n",
    "    )\n",
    "\n",
    "    # we need special queue and value objects\n",
    "    # specific to process spawning\n",
    "    smp = torch.multiprocessing.get_context(\"spawn\")\n",
    "    q = smp.Queue()\n",
    "    sync = smp.Value(\"d\", 0.0)\n",
    "\n",
    "    # pass a bunch of arguments into each\n",
    "    # process that we need to spawn\n",
    "    # note that we have to pass copies of some\n",
    "    # of our local functions that live in `utils`\n",
    "    # since we can't unpickle elsewhere functions\n",
    "    # which are defined in __main__\n",
    "    args = (\n",
    "        X,  # the full dataset\n",
    "        utils.MLP,  # the module class to use for inference\n",
    "        [INPUT_SIZE, HIDDEN_SIZES],  # arguments to initialize the module\n",
    "        utils.do_some_inference,  # the inference funcntion to use\n",
    "        q,  # the queue to put the results in\n",
    "        sync,  # a task synchronizer\n",
    "        jobs_per_gpu,\n",
    "        num_gpus\n",
    "    )\n",
    "\n",
    "    # spawn parallel jobs across all GPUs.\n",
    "    # We have to host the `parallel_inference_task` function\n",
    "    # in a separate module for the same pickling pickle\n",
    "    # described above\n",
    "    procs = torch.multiprocessing.spawn(\n",
    "        utils.parallel_inference_task,\n",
    "        args=args,\n",
    "        nprocs=num_jobs,\n",
    "        join=False\n",
    "    )\n",
    "\n",
    "    # wait to synchronize until all models load\n",
    "    # so that we can compare throughput better\n",
    "    while sync.value < num_jobs:\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # increment the synchronizer one more\n",
    "    # time to kick off the jobs\n",
    "    sync.value += 1\n",
    "    progbar.start_task(task_id)\n",
    "\n",
    "    # collect all the (unordered) inputs\n",
    "    outputs = []\n",
    "    while not (progbar.finished and procs.join(0.01)):\n",
    "        try:\n",
    "            # try to get the next result in\n",
    "            # in the queue and increment everything\n",
    "            outputs.extend(q.get_nowait())\n",
    "            progbar.update(task_id, completed=len(outputs))\n",
    "        except Empty:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    # concatenate the outputs and return\n",
    "    return np.stack(outputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6fcf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does GPU usage and time-to-completion scale with the number of jobs/GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1355e87",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4d8be1095144c0b728107beb9c9a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with utils.GpuUtilProgress([0, 1]) as progbar:\n",
    "    y = parallel_inference(dataset, num_gpus=1, jobs_per_gpu=2, progbar=progbar)\n",
    "    y = parallel_inference(dataset, num_gpus=1, jobs_per_gpu=4, progbar=progbar)\n",
    "    y = parallel_inference(dataset, num_gpus=2, jobs_per_gpu=4, progbar=progbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425f180",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Things seem to scale pretty well! But this is still far from ideal:\n",
    "- Framework specific\n",
    "    - No help if we want to extend to other frameworks\n",
    "    - Torch is pretty unique in having this functionality at all\n",
    "- The code is complicated and required a lot of non-physics expertise to build\n",
    "    - Non-trivial to reconstruct for new applications\n",
    "- Extremely contrived example, breaks down in most real use cases\n",
    "    - Explore a few cases to show how"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3aa3e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Throughput too low\n",
    "#### _The constraints of our use case demand that we further reduce processing time by an order of magnitude_\n",
    "- Not obvious how to simply extend this code to multi-node\n",
    "- Scaling not dynamic\n",
    "    - Have to pick a level of parallelism and hope the resources are available to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b65e07",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Throughput too high\n",
    "####  _Data generation process is slow, needs to be parallelized to saturate GPU throughput_\n",
    "- Low GPU utilization with local resources. Not obvious how this code can:\n",
    "    - Scale to multiple clients\n",
    "    - Allow other users to leverage spare cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a1bd6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model ensembling\n",
    "#### _Connecting multiple models in a single pipeline_\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/model_sharing.png\" height=\"auto\" width=\"400px\" class=\"center\" />\n",
    "    <img src=\"images/model_ensemble.png\" height=\"auto\" width=\"400px\" class=\"center\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e8efa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model ensembling\n",
    "Naive implemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25a52d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c99c4133b046a5bb0c1a4b439b17e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def do_some_multi_model_inference(models, dataset, batch_size=8, device_index=0):\n",
    "    gpu_dataset = torch.from_numpy(dataset).cuda(device_index)\n",
    "    dataset = torch.utils.data.TensorDataset(gpu_dataset)\n",
    "\n",
    "    for [x] in torch.utils.data.DataLoader(dataset, batch_size=batch_size):\n",
    "        for model in models:\n",
    "            x = model(x)\n",
    "        yield x.cpu().numpy()\n",
    "\n",
    "noise_remover = utils.NoiseRemovalModel(INPUT_SIZE, [32, 16]).cuda(0)\n",
    "models = [noise_remover, inference_model]\n",
    "\n",
    "with utils.GpuUtilProgress(0) as progbar:\n",
    "    task_id = progbar.add_task(\"[cyan]Ensemble inference\", total=N)\n",
    "    for y in do_some_multi_model_inference(models, dataset):\n",
    "        progbar.update(task_id, advance=len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33b6c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Ensembling\n",
    "\n",
    "Once again, it _works_, but scaling it up is non-trivial:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9b050",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Models may require different levels of parallelism to keep any one model from bottlenecking the other (see figure)\n",
    "- Most efficient implementation would have models executing asynchronously, with tensors passed between GPUs\n",
    "- If the models utilize different frameworks, this problem becomes exponentially harder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38102150",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<figure>\n",
    "    <img src=\"images/bottleneck-both.png\" height=\"auto\" width=\"800px\"/>\n",
    "    <figcaption>Model 1 throughput too high for model 2, need to run more concurrent instances of model 2 to maximize throughput</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00c689",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distribution\n",
    "#### _Who do you want to use your model?_\n",
    "\n",
    "- How much expertise should someone need to have to utilize your model in their pipeline?\n",
    "- How much do they need to know about how your model is implemented?\n",
    "- How will they be kept up-to-date when you retrain the model or improve the architecture?\n",
    "    - Will these updates change their pipeline?\n",
    "- What if they don't have access to accelerators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046926a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference-as-a-Service\n",
    "### An alternative paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dfc24a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Takeaways so far:\n",
    "- Efficiently scheduling cross-platform, multi-GPU, multi-model asynchronous DL inference is hard\n",
    "- Inference is just one piece of your pipeline. Really even just one line:\n",
    "```python\n",
    "y = model(x)\n",
    "```\n",
    "    How and where `model(x)` happens is largely irrelevant to everything else\n",
    "\n",
    "So:\n",
    "- Manage this piece separately to hide these details\n",
    "- Scale it to meet the rate at which you can generate `x`s or how quickly you need `y`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952356a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inference-as-a-Service\n",
    "The **inference-as-a-service** (Iaas) paradigm addresses these issues\n",
    "- Out-of-the-box software optimized for efficiently executing complex asynchronous workloads across devices\n",
    "    - Hardware _and_ framework agnostic\n",
    "- Exposes models for inference to **client** pipelines via standardized APIs\n",
    "    - Pipeline code stays the same even as the model changes or the service moves\n",
    "    - Centralized model repositories keep all clients on the same page\n",
    "- Containerization makes deployments portable to meet workload demands\n",
    "    - Minimizes environment management overhead\n",
    "    - Integration with container management servicse like Kubernetes leads to easy scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7be1b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Traditional pipeline pseudocode\n",
    "```python\n",
    "# need to define the architecture somewhere\n",
    "# that users can get access to it\n",
    "from model_zoo import MyModel\n",
    "\n",
    "# load in some parameters to make sure we\n",
    "# initialize the model correctly\n",
    "with open(\"path/to/init/args.pickle\", \"rb\") as f:\n",
    "    args = pickle.load(f)\n",
    "model = MyModel(**args)\n",
    "\n",
    "# load in the latest checkpoint we know of\n",
    "model.load_weights(\"path/to/latest/weights.h5\")\n",
    "\n",
    "for x in dataset:\n",
    "    # this syntax will differ depending on the framework\n",
    "    x = move_array_to_gpu(x)\n",
    "    y = model(x)\n",
    "    y = move_output_to_cpu(y)\n",
    "    do_downstream_postprocessing(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a35df9",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Iaas\n",
    "```python\n",
    "import tritonclient.grpc as triton\n",
    "\n",
    "# connect to the service at some url\n",
    "# pipeline never needs to touch model itself\n",
    "client = triton.InferenceServerClient(\"0.0.0.0:8001\")\n",
    "\n",
    "# build a protobuf message representing the input\n",
    "metadata = client.get_model_metadata(\"my_model\").inputs[0]\n",
    "input = triton.InferInput(\n",
    "    metadata.name, metadata.shape, metadata.datatype\n",
    ")\n",
    "\n",
    "for x in dataset:\n",
    "    input.set_data_from_numpy(x)\n",
    "\n",
    "    # use the latest available version of the model\n",
    "    y = client.infer(\"my_model\", inputs=[input])\n",
    "    do_downstream_postprocessing(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cb114",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The difference is that, _as is_, this code gets you:\n",
    "- As much scale as you want\n",
    "- on whatever hardware you want\n",
    "- using whatever backend framework you want\n",
    "- wherever you want\n",
    "- and can receive updates without interrupting service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd99ee",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "```python\n",
    "import tritonclient.grpc as triton\n",
    "\n",
    "# connect to the service at some url\n",
    "# pipeline never needs to touch model itself\n",
    "client = triton.InferenceServerClient(\"0.0.0.0:8001\")\n",
    "\n",
    "# build a protobuf message representing the input\n",
    "metadata = client.get_model_metadata(\"my_model\").inputs[0]\n",
    "input = triton.InferInput(\n",
    "    metadata.name, metadata.shape, metadata.datatype\n",
    ")\n",
    "\n",
    "for x in dataset:\n",
    "    input.set_data_from_numpy(x)\n",
    "\n",
    "    # use the latest available version of the model\n",
    "    y = client.infer(\"my_model\", inputs=[input])\n",
    "    do_downstream_postprocessing(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9154848",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deployment scenarios\n",
    "- IaaS represents a _software_ model for managing inference execution on heterogenous _hardware_\n",
    "- Not tied to any _particular_ hardware platform or deployment location\n",
    "    - Tune to meet the needs of each use case\n",
    "\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/triton-ldg.png\" height=\"auto\" width=\"350px\" class=\"left\" />\n",
    "    <img src=\"images/triton-cloud.png\" height=\"auto\" width=\"350px\" class=\"right\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff55bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Triton Inference Server](https://github.com/triton-inference-server/server)\n",
    "Off-the-shelf inference service developed and maintained by NVIDIA.\n",
    "- Efficient scheduling of GPU resources\n",
    "- Multiple framework backend support\n",
    "    - Pre-built containers released monthly to simplify dependency management\n",
    "- Dynamic model versioning and ensemble scheduling\n",
    "- Separately installable [client libraries](https://github.com/triton-inference-server/client)\n",
    "\n",
    "![Find a triton image](images/triton-logo.png \"Find a triton image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ce39f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## End-to-end example\n",
    "- A working example starting from model export and working up to multi-node ensemble inference in the cloud\n",
    "- Discuss key concepts of IaaS, Triton, and cloud computing along the way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d78eb2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exporting models\n",
    "Things we need for each model we want to use for inference\n",
    "> - The set of operations that constitute the model\n",
    "> - The optimized (trained) set of model parameters or \"weights\"\n",
    "\n",
    "Triton expects these objects to be stored in a single location: the **model repository**\n",
    "- Local filesystem or cloud storage location\n",
    "- Expected to have fixed structure denoting different models and their various versions\n",
    "- Models are loaded from the repository into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4ec56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model repository\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/repo-outline.png\" height=\"auto\" width=\"800px\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68687d62",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Repo conventions can be tedious and redundant\n",
    "    - Built around protobuf - syntax tricky to learn\n",
    "- We've built a wrapper library `quiver` to simplify these headaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be016d35",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"my-model\"\n",
       "platform: \"onnxruntime_onnx\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gravswell import quiver as qv\n",
    "\n",
    "# represents a model repo on the local filesystem\n",
    "repo = qv.ModelRepository(\"my-repo\")\n",
    "\n",
    "# add a blank model entry to the repo\n",
    "entry = repo.add(\"my-model\", platform=qv.Platform.ONNX)\n",
    "\n",
    "# see how this looks visually\n",
    "utils.print_tree(\"my-repo\")\n",
    "\n",
    "print(\"Model config:\")\n",
    "entry.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d139b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Export an initial version of our model\n",
    "- Combines the operations and weights into a single representation using [ONNX](https://github.com/onnx)\n",
    "- Specify the names and sizes of the tensors going in and out of the model\n",
    "    - Output sizes are inferred by running the model on input tensors of the specified shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb88bf8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"my-model\"\n",
       "platform: \"onnxruntime_onnx\"\n",
       "input {\n",
       "  name: \"x\"\n",
       "  data_type: TYPE_FP32\n",
       "  dims: -1\n",
       "  dims: 64\n",
       "}\n",
       "output {\n",
       "  name: \"y\"\n",
       "  data_type: TYPE_FP32\n",
       "  dims: -1\n",
       "  dims: 1\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move the model back to the CPU for output shape inference\n",
    "inference_model.to(\"cpu\")\n",
    "\n",
    "# export this version of the model\n",
    "export_path = entry.export_version(\n",
    "    inference_model,\n",
    "    input_shapes={\"x\": (None, INPUT_SIZE)},\n",
    "    output_names=[\"y\"]\n",
    ")\n",
    "\n",
    "# inputs and outputs are dynamically added to the config\n",
    "entry.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1fe10",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now what does our model repo look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8cd78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-repo/\n",
      "    my-model/\n",
      "        1/\n",
      "            model.onnx\n",
      "        config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "utils.print_tree(\"my-repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797a81e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For ensuing versions of our model, don't need to specify anything.\n",
    "    - If we do, it will be compared against the config to make sure they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8451e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-repo/\n",
      "    my-model/\n",
      "        1/\n",
      "            model.onnx\n",
      "        2/\n",
      "            model.onnx\n",
      "        config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "# do_some_more_training(model, new_train_dataset)\n",
    "# export this even-more-optimized version 2.0 of the model\n",
    "entry.export_version(inference_model)\n",
    "\n",
    "# what does this look like now?\n",
    "utils.print_tree(\"my-repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa0be7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parallel Inference\n",
    "- Triton can host multiple copies of our model on a single GPU at once\n",
    "    - Allows for easy parallel execution\n",
    "    - Can be scaled per-model, per-gpu\n",
    "    - Described in config **instance group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42cb4e70",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"my-model\"\n",
       "platform: \"onnxruntime_onnx\"\n",
       "input {\n",
       "  name: \"x\"\n",
       "  data_type: TYPE_FP32\n",
       "  dims: -1\n",
       "  dims: 64\n",
       "}\n",
       "output {\n",
       "  name: \"y\"\n",
       "  data_type: TYPE_FP32\n",
       "  dims: -1\n",
       "  dims: 1\n",
       "}\n",
       "instance_group {\n",
       "  count: 4\n",
       "  kind: KIND_GPU\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# host 4 parallel inference executions on _all_ available GPUs\n",
    "entry.config.add_instance_group(count=4)\n",
    "entry.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24562170",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cloud storage\n",
    "- Local filesystem not helpful if we choose to host our inference service on a remote server\n",
    "- Use a **cloud storage** bucket on Google Cloud to host the model repository\n",
    "- Accessible via web request APIs from anywhere (with authentication)\n",
    "- `quiver` supports natively without changing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61509226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ligo-quiver-demo/my-model/config.pbtxt\r\n",
      "gs://ligo-quiver-demo/my-model/1/\r\n"
     ]
    }
   ],
   "source": [
    "# get rid of local repository\n",
    "repo.delete()\n",
    "\n",
    "# use gs:// prefix to indicate Google Cloud bucket\n",
    "repo = qv.ModelRepository(\"gs://ligo-quiver-demo\")\n",
    "\n",
    "# everything else proceeds as normal\n",
    "entry = repo.add(\"my-model\", platform=qv.Platform.ONNX)\n",
    "entry.config.add_instance_group(count=4)\n",
    "export_path = entry.export_version(\n",
    "    inference_model,\n",
    "    input_shapes={\"x\": (None, INPUT_SIZE)},\n",
    "    output_names=[\"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49661b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use Google's command line utility to confirm bucket contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://ligo-quiver-demo/my-model/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ab992",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deploying on the cloud\n",
    "- Now our model has been exported to a model repository, we can deploy a Triton server instance that loads our model from it\n",
    "- To do this we'll use a **Kubernetes** cluster deployed on Google Cloud\n",
    "    - Reserve a pool of resoures sitting on servers in a Google data center\n",
    "    - Schedule workloads on those resources using **Docker** containers\n",
    "        - Extremely lightweight VMs containing our Triton environment and executable\n",
    "    - Kubernetes intelligently manages deployed clusters and exposes them to external requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8a706",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deploying on the cloud\n",
    "We've built another library, `cloudbreak` to help make managing these deployments simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dba2d38e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66363592f234a4e968a2c6bd9c2600d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gravswell.cloudbreak import google as cb\n",
    "\n",
    "# object for creating/destroying clusters associated\n",
    "# with a particular **project** (id used for billing)\n",
    "# in a particular **zone** (identifies a datacenter\n",
    "# location. We'll use the same one this VM is running\n",
    "# in, somwhere in Virginia)\n",
    "manager = cb.ClusterManager(project=\"gunny-multi-instance-dev\", zone=\"us-east4-b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f9f5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import container_v1 as container\n",
    "\n",
    "# create a description of a newly initialized\n",
    "# cluster. Give it a single vanilla node to run\n",
    "# the kubernetes manager on\n",
    "cluster_config = container.Cluster(\n",
    "    name=\"ligo-demo-cluster\",\n",
    "    node_pools=[container.NodePool(\n",
    "        name=\"default-pool\",\n",
    "        initial_node_count=1,\n",
    "        config=container.NodeConfig()\n",
    "    )]\n",
    ")\n",
    "\n",
    "# initialize this \"blank\" cluster in the\n",
    "# specified zone, with its resource usage\n",
    "# billed to the specified project\n",
    "cluster = manager.create_resource(cluster_config)\n",
    "\n",
    "# we're going to use GPUs on this cluster, so\n",
    "# run a utility script which makes sure that\n",
    "# the NVIDIA drivers will be installed on any\n",
    "# GPU-enabled nodes we might create\n",
    "cluster.deploy_gpu_drivers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75885a88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have a cluster with a node dedicated to running our Kubernetes deployment, attach some GPU-enabled nodes that will be controlled by Kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eac98e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60677582f5fe418c89518faf2550bc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# describe what these GPU-enabled nodes should look like\n",
    "# and how many we want\n",
    "node_pool_config = container.NodePool(\n",
    "    name=\"tritonserver-t4-pool\",\n",
    "    initial_node_count=2,\n",
    "    config=cb.create_gpu_node_pool_config(\n",
    "        vcpus=16,\n",
    "        gpus=4,\n",
    "        gpu_type=\"t4\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# add these nodes to our cluster\n",
    "node_pool = cluster.create_resource(node_pool_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cdbbb9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deploying Triton with Kubernetes\n",
    "- Kubernetes operates by using config YAML files to describe the desired _state_ of a deployment. This includes:\n",
    "    - The name of the deployment\n",
    "    - The Docker container(s) needed to run it\n",
    "    - The command to run inside those containers\n",
    "    - What types of node are acceptable for deploying on\n",
    "    - How to expose the deployment to requests\n",
    "- Let's take a look at our Triton deployment file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b8455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #408080; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #BC7A00 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #FF0000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #00A000 } /* Generic.Inserted */\n",
       ".output_html .go { color: #888888 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #7D9029 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #A0A000 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #BB6688 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"nt\">apiVersion</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">apps/v1</span>\n",
       "<span class=\"nt\">kind</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">Deployment</span>\n",
       "<span class=\"nt\">metadata</span><span class=\"p\">:</span>\n",
       "  <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{{</span> <span class=\"nv\">.Values.name</span> <span class=\"p p-Indicator\">}}</span>\n",
       "  <span class=\"nt\">namespace</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">default</span>\n",
       "<span class=\"nt\">spec</span><span class=\"p\">:</span>\n",
       "  <span class=\"nt\">replicas</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n",
       "  <span class=\"nt\">selector</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">matchLabels</span><span class=\"p\">:</span>\n",
       "      <span class=\"nt\">app</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{{</span> <span class=\"nv\">.Values.name</span> <span class=\"p p-Indicator\">}}</span>\n",
       "  <span class=\"nt\">progressDeadlineSeconds</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">100</span>\n",
       "  <span class=\"nt\">template</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">metadata</span><span class=\"p\">:</span>\n",
       "      <span class=\"nt\">labels</span><span class=\"p\">:</span>\n",
       "        <span class=\"nt\">app</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{{</span> <span class=\"nv\">.Values.name</span> <span class=\"p p-Indicator\">}}</span>\n",
       "    <span class=\"nt\">spec</span><span class=\"p\">:</span>\n",
       "      <span class=\"nt\">securityContext</span><span class=\"p\">:</span>\n",
       "        <span class=\"nt\">runAsUser</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1000</span>\n",
       "        <span class=\"nt\">fsGroup</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1000</span>\n",
       "      <span class=\"nt\">containers</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">tritonserver</span>\n",
       "        <span class=\"nt\">image</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">nvcr.io/nvidia/tritonserver:{{ .Values.tag }}-py3</span>\n",
       "        <span class=\"nt\">env</span><span class=\"p\">:</span>\n",
       "        <span class=\"p p-Indicator\">-</span> <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">EXTRA_NV_PATHS</span>\n",
       "          <span class=\"nt\">value</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">/usr/local/nvidia/lib64:/usr/local/nvidia/bin</span>\n",
       "        <span class=\"nt\">command</span><span class=\"p\">:</span>\n",
       "        <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">/bin/bash</span>\n",
       "        <span class=\"nt\">args</span><span class=\"p\">:</span>\n",
       "        <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">-c</span>\n",
       "        <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:$EXTRA_NV_PATHS&quot; bin/tritonserver --model-repository gs://{{ .Values.bucket }} --repository-poll-secs 30 --model-control-mode explicit</span>\n",
       "        <span class=\"nt\">resources</span><span class=\"p\">:</span>\n",
       "          <span class=\"nt\">limits</span><span class=\"p\">:</span>\n",
       "            <span class=\"nt\">nvidia.com/gpu</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{{</span> <span class=\"nv\">.Values.gpus</span> <span class=\"p p-Indicator\">}}</span>\n",
       "            <span class=\"nt\">cpu</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{{</span> <span class=\"nv\">.Values.vcpus</span> <span class=\"p p-Indicator\">}}</span>\n",
       "        <span class=\"nt\">ports</span><span class=\"p\">:</span>\n",
       "        <span class=\"p p-Indicator\">-</span> <span class=\"nt\">containerPort</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8000</span>\n",
       "          <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">http</span>\n",
       "        <span class=\"p p-Indicator\">-</span> <span class=\"nt\">containerPort</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8001</span>\n",
       "          <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">grpc</span>\n",
       "        <span class=\"p p-Indicator\">-</span> <span class=\"nt\">containerPort</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8002</span>\n",
       "          <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">metrics</span>\n",
       "        <span class=\"nt\">livenessProbe</span><span class=\"p\">:</span>\n",
       "          <span class=\"nt\">failureThreshold</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">30</span>\n",
       "          <span class=\"nt\">initialDelaySeconds</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">100</span>\n",
       "          <span class=\"nt\">periodSeconds</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">5</span>\n",
       "          <span class=\"nt\">httpGet</span><span class=\"p\">:</span>\n",
       "            <span class=\"nt\">path</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">/v2/health/live</span>\n",
       "            <span class=\"nt\">port</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">http</span>\n",
       "        <span class=\"nt\">startupProbe</span><span class=\"p\">:</span>\n",
       "          <span class=\"nt\">failureThreshold</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">30</span>\n",
       "          <span class=\"nt\">periodSeconds</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n",
       "          <span class=\"nt\">httpGet</span><span class=\"p\">:</span>\n",
       "            <span class=\"nt\">path</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">/v2/health/ready</span>\n",
       "            <span class=\"nt\">port</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">http</span>\n",
       "        <span class=\"nt\">readinessProbe</span><span class=\"p\">:</span>\n",
       "          <span class=\"nt\">failureThreshold</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">30</span>\n",
       "          <span class=\"nt\">periodSeconds</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n",
       "          <span class=\"nt\">httpGet</span><span class=\"p\">:</span>\n",
       "            <span class=\"nt\">path</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">/v2/health/ready</span>\n",
       "            <span class=\"nt\">port</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">http</span>\n",
       "      <span class=\"nt\">nodeSelector</span><span class=\"p\">:</span>\n",
       "        <span class=\"nt\">cloud.google.com/gke-accelerator</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">nvidia-tesla-t4</span>\n",
       "<span class=\"nn\">---</span>\n",
       "<span class=\"nt\">apiVersion</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">v1</span>\n",
       "<span class=\"nt\">kind</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">Service</span>\n",
       "<span class=\"nt\">metadata</span><span class=\"p\">:</span>\n",
       "  <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{{</span> <span class=\"nv\">.Values.name</span> <span class=\"p p-Indicator\">}}</span>\n",
       "  <span class=\"nt\">namespace</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">default</span>\n",
       "<span class=\"nt\">spec</span><span class=\"p\">:</span>\n",
       "  <span class=\"c1\">#externalTrafficPolicy: Cluster</span>\n",
       "  <span class=\"nt\">ports</span><span class=\"p\">:</span>\n",
       "  <span class=\"p p-Indicator\">-</span> <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">http-triton</span>\n",
       "    <span class=\"nt\">port</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8000</span>\n",
       "    <span class=\"nt\">protocol</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">TCP</span>\n",
       "    <span class=\"nt\">targetPort</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8000</span>\n",
       "  <span class=\"p p-Indicator\">-</span> <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">grpc-triton</span>\n",
       "    <span class=\"nt\">port</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8001</span>\n",
       "    <span class=\"nt\">protocol</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">TCP</span>\n",
       "    <span class=\"nt\">targetPort</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8001</span>\n",
       "  <span class=\"p p-Indicator\">-</span> <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">metrics-triton</span>\n",
       "    <span class=\"nt\">port</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8002</span>\n",
       "    <span class=\"nt\">protocol</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">TCP</span>\n",
       "    <span class=\"nt\">targetPort</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">8002</span>\n",
       "  <span class=\"nt\">selector</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">app</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{{</span> <span class=\"nv\">.Values.name</span> <span class=\"p p-Indicator\">}}</span>\n",
       "  <span class=\"nt\">sessionAffinity</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">None</span>\n",
       "  <span class=\"nt\">type</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">LoadBalancer</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{n+nt}{apiVersion}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{apps/v1}\n",
       "\\PY{n+nt}{kind}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{Deployment}\n",
       "\\PY{n+nt}{metadata}\\PY{p}{:}\n",
       "  \\PY{n+nt}{name}\\PY{p}{:} \\PY{p+pIndicator}{\\PYZob{}}\\PY{p+pIndicator}{\\PYZob{}} \\PY{n+nv}{.Values.name} \\PY{p+pIndicator}{\\PYZcb{}}\\PY{p+pIndicator}{\\PYZcb{}}\n",
       "  \\PY{n+nt}{namespace}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{default}\n",
       "\\PY{n+nt}{spec}\\PY{p}{:}\n",
       "  \\PY{n+nt}{replicas}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{1}\n",
       "  \\PY{n+nt}{selector}\\PY{p}{:}\n",
       "    \\PY{n+nt}{matchLabels}\\PY{p}{:}\n",
       "      \\PY{n+nt}{app}\\PY{p}{:} \\PY{p+pIndicator}{\\PYZob{}}\\PY{p+pIndicator}{\\PYZob{}} \\PY{n+nv}{.Values.name} \\PY{p+pIndicator}{\\PYZcb{}}\\PY{p+pIndicator}{\\PYZcb{}}\n",
       "  \\PY{n+nt}{progressDeadlineSeconds}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{100}\n",
       "  \\PY{n+nt}{template}\\PY{p}{:}\n",
       "    \\PY{n+nt}{metadata}\\PY{p}{:}\n",
       "      \\PY{n+nt}{labels}\\PY{p}{:}\n",
       "        \\PY{n+nt}{app}\\PY{p}{:} \\PY{p+pIndicator}{\\PYZob{}}\\PY{p+pIndicator}{\\PYZob{}} \\PY{n+nv}{.Values.name} \\PY{p+pIndicator}{\\PYZcb{}}\\PY{p+pIndicator}{\\PYZcb{}}\n",
       "    \\PY{n+nt}{spec}\\PY{p}{:}\n",
       "      \\PY{n+nt}{securityContext}\\PY{p}{:}\n",
       "        \\PY{n+nt}{runAsUser}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{1000}\n",
       "        \\PY{n+nt}{fsGroup}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{1000}\n",
       "      \\PY{n+nt}{containers}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{tritonserver}\n",
       "        \\PY{n+nt}{image}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{nvcr.io/nvidia/tritonserver:\\PYZob{}\\PYZob{}}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{.Values.tag}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{\\PYZcb{}\\PYZcb{}\\PYZhy{}py3}\n",
       "        \\PY{n+nt}{env}\\PY{p}{:}\n",
       "        \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{EXTRA\\PYZus{}NV\\PYZus{}PATHS}\n",
       "          \\PY{n+nt}{value}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{/usr/local/nvidia/lib64:/usr/local/nvidia/bin}\n",
       "        \\PY{n+nt}{command}\\PY{p}{:}\n",
       "        \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{/bin/bash}\n",
       "        \\PY{n+nt}{args}\\PY{p}{:}\n",
       "        \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{\\PYZhy{}c}\n",
       "        \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{LD\\PYZus{}LIBRARY\\PYZus{}PATH=\\PYZdq{}\\PYZdl{}LD\\PYZus{}LIBRARY\\PYZus{}PATH:\\PYZdl{}EXTRA\\PYZus{}NV\\PYZus{}PATHS\\PYZdq{}}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{bin/tritonserver}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{\\PYZhy{}\\PYZhy{}model\\PYZhy{}repository}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{gs://\\PYZob{}\\PYZob{}}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{.Values.bucket}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{\\PYZcb{}\\PYZcb{}}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{\\PYZhy{}\\PYZhy{}repository\\PYZhy{}poll\\PYZhy{}secs}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{30}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{\\PYZhy{}\\PYZhy{}model\\PYZhy{}control\\PYZhy{}mode}\\PY{l+lScalar+lScalarPlain}{ }\\PY{l+lScalar+lScalarPlain}{explicit}\n",
       "        \\PY{n+nt}{resources}\\PY{p}{:}\n",
       "          \\PY{n+nt}{limits}\\PY{p}{:}\n",
       "            \\PY{n+nt}{nvidia.com/gpu}\\PY{p}{:} \\PY{p+pIndicator}{\\PYZob{}}\\PY{p+pIndicator}{\\PYZob{}} \\PY{n+nv}{.Values.gpus} \\PY{p+pIndicator}{\\PYZcb{}}\\PY{p+pIndicator}{\\PYZcb{}}\n",
       "            \\PY{n+nt}{cpu}\\PY{p}{:} \\PY{p+pIndicator}{\\PYZob{}}\\PY{p+pIndicator}{\\PYZob{}} \\PY{n+nv}{.Values.vcpus} \\PY{p+pIndicator}{\\PYZcb{}}\\PY{p+pIndicator}{\\PYZcb{}}\n",
       "        \\PY{n+nt}{ports}\\PY{p}{:}\n",
       "        \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{containerPort}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8000}\n",
       "          \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{http}\n",
       "        \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{containerPort}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8001}\n",
       "          \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{grpc}\n",
       "        \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{containerPort}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8002}\n",
       "          \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{metrics}\n",
       "        \\PY{n+nt}{livenessProbe}\\PY{p}{:}\n",
       "          \\PY{n+nt}{failureThreshold}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{30}\n",
       "          \\PY{n+nt}{initialDelaySeconds}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{100}\n",
       "          \\PY{n+nt}{periodSeconds}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{5}\n",
       "          \\PY{n+nt}{httpGet}\\PY{p}{:}\n",
       "            \\PY{n+nt}{path}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{/v2/health/live}\n",
       "            \\PY{n+nt}{port}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{http}\n",
       "        \\PY{n+nt}{startupProbe}\\PY{p}{:}\n",
       "          \\PY{n+nt}{failureThreshold}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{30}\n",
       "          \\PY{n+nt}{periodSeconds}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{10}\n",
       "          \\PY{n+nt}{httpGet}\\PY{p}{:}\n",
       "            \\PY{n+nt}{path}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{/v2/health/ready}\n",
       "            \\PY{n+nt}{port}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{http}\n",
       "        \\PY{n+nt}{readinessProbe}\\PY{p}{:}\n",
       "          \\PY{n+nt}{failureThreshold}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{30}\n",
       "          \\PY{n+nt}{periodSeconds}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{10}\n",
       "          \\PY{n+nt}{httpGet}\\PY{p}{:}\n",
       "            \\PY{n+nt}{path}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{/v2/health/ready}\n",
       "            \\PY{n+nt}{port}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{http}\n",
       "      \\PY{n+nt}{nodeSelector}\\PY{p}{:}\n",
       "        \\PY{n+nt}{cloud.google.com/gke\\PYZhy{}accelerator}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{nvidia\\PYZhy{}tesla\\PYZhy{}t4}\n",
       "\\PY{n+nn}{\\PYZhy{}\\PYZhy{}\\PYZhy{}}\n",
       "\\PY{n+nt}{apiVersion}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{v1}\n",
       "\\PY{n+nt}{kind}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{Service}\n",
       "\\PY{n+nt}{metadata}\\PY{p}{:}\n",
       "  \\PY{n+nt}{name}\\PY{p}{:} \\PY{p+pIndicator}{\\PYZob{}}\\PY{p+pIndicator}{\\PYZob{}} \\PY{n+nv}{.Values.name} \\PY{p+pIndicator}{\\PYZcb{}}\\PY{p+pIndicator}{\\PYZcb{}}\n",
       "  \\PY{n+nt}{namespace}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{default}\n",
       "\\PY{n+nt}{spec}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{}externalTrafficPolicy: Cluster}\n",
       "  \\PY{n+nt}{ports}\\PY{p}{:}\n",
       "  \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{http\\PYZhy{}triton}\n",
       "    \\PY{n+nt}{port}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8000}\n",
       "    \\PY{n+nt}{protocol}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{TCP}\n",
       "    \\PY{n+nt}{targetPort}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8000}\n",
       "  \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{grpc\\PYZhy{}triton}\n",
       "    \\PY{n+nt}{port}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8001}\n",
       "    \\PY{n+nt}{protocol}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{TCP}\n",
       "    \\PY{n+nt}{targetPort}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8001}\n",
       "  \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{name}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{metrics\\PYZhy{}triton}\n",
       "    \\PY{n+nt}{port}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8002}\n",
       "    \\PY{n+nt}{protocol}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{TCP}\n",
       "    \\PY{n+nt}{targetPort}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{8002}\n",
       "  \\PY{n+nt}{selector}\\PY{p}{:}\n",
       "    \\PY{n+nt}{app}\\PY{p}{:} \\PY{p+pIndicator}{\\PYZob{}}\\PY{p+pIndicator}{\\PYZob{}} \\PY{n+nv}{.Values.name} \\PY{p+pIndicator}{\\PYZcb{}}\\PY{p+pIndicator}{\\PYZcb{}}\n",
       "  \\PY{n+nt}{sessionAffinity}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{None}\n",
       "  \\PY{n+nt}{type}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{LoadBalancer}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "apiVersion: apps/v1\n",
       "kind: Deployment\n",
       "metadata:\n",
       "  name: {{ .Values.name }}\n",
       "  namespace: default\n",
       "spec:\n",
       "  replicas: 1\n",
       "  selector:\n",
       "    matchLabels:\n",
       "      app: {{ .Values.name }}\n",
       "  progressDeadlineSeconds: 100\n",
       "  template:\n",
       "    metadata:\n",
       "      labels:\n",
       "        app: {{ .Values.name }}\n",
       "    spec:\n",
       "      securityContext:\n",
       "        runAsUser: 1000\n",
       "        fsGroup: 1000\n",
       "      containers:\n",
       "      - name: tritonserver\n",
       "        image: nvcr.io/nvidia/tritonserver:{{ .Values.tag }}-py3\n",
       "        env:\n",
       "        - name: EXTRA_NV_PATHS\n",
       "          value: /usr/local/nvidia/lib64:/usr/local/nvidia/bin\n",
       "        command:\n",
       "        - /bin/bash\n",
       "        args:\n",
       "        - -c\n",
       "        - LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$EXTRA_NV_PATHS\" bin/tritonserver --model-repository gs://{{ .Values.bucket }} --repository-poll-secs 30 --model-control-mode explicit\n",
       "        resources:\n",
       "          limits:\n",
       "            nvidia.com/gpu: {{ .Values.gpus }}\n",
       "            cpu: {{ .Values.vcpus }}\n",
       "        ports:\n",
       "        - containerPort: 8000\n",
       "          name: http\n",
       "        - containerPort: 8001\n",
       "          name: grpc\n",
       "        - containerPort: 8002\n",
       "          name: metrics\n",
       "        livenessProbe:\n",
       "          failureThreshold: 30\n",
       "          initialDelaySeconds: 100\n",
       "          periodSeconds: 5\n",
       "          httpGet:\n",
       "            path: /v2/health/live\n",
       "            port: http\n",
       "        startupProbe:\n",
       "          failureThreshold: 30\n",
       "          periodSeconds: 10\n",
       "          httpGet:\n",
       "            path: /v2/health/ready\n",
       "            port: http\n",
       "        readinessProbe:\n",
       "          failureThreshold: 30\n",
       "          periodSeconds: 10\n",
       "          httpGet:\n",
       "            path: /v2/health/ready\n",
       "            port: http\n",
       "      nodeSelector:\n",
       "        cloud.google.com/gke-accelerator: nvidia-tesla-t4\n",
       "---\n",
       "apiVersion: v1\n",
       "kind: Service\n",
       "metadata:\n",
       "  name: {{ .Values.name }}\n",
       "  namespace: default\n",
       "spec:\n",
       "  #externalTrafficPolicy: Cluster\n",
       "  ports:\n",
       "  - name: http-triton\n",
       "    port: 8000\n",
       "    protocol: TCP\n",
       "    targetPort: 8000\n",
       "  - name: grpc-triton\n",
       "    port: 8001\n",
       "    protocol: TCP\n",
       "    targetPort: 8001\n",
       "  - name: metrics-triton\n",
       "    port: 8002\n",
       "    protocol: TCP\n",
       "    targetPort: 8002\n",
       "  selector:\n",
       "    app: {{ .Values.name }}\n",
       "  sessionAffinity: None\n",
       "  type: LoadBalancer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Code\n",
    "Code(filename=\"triton.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b43e3b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `cloudbreak.Cluster` object takes care of making the deployment request\n",
    "- Fills in the `{{ .Values.* }}` wildcards for flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de22b31b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4462d4ebc44b94bcfc74f68a63f61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster.deploy(\n",
    "    \"triton.yaml\",\n",
    "    name=\"tritonserver\",\n",
    "    tag=\"20.11\",\n",
    "    bucket=\"ligo-quiver-demo\",\n",
    "    gpus=4,\n",
    "    vcpus=15  # at least come cpu has to go to running kubernetes\n",
    ")\n",
    "cluster.k8s_client.wait_for_deployment(\"tritonserver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae4463",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quick recap\n",
    "Now we have a model exported to a Google Cloud Storage bucket, that's being loaded into memory by a Triton server instance, which is running on a Google Cloud server node, being managed by Kubernetes, and exposed to external requests by a load balancer.\n",
    "\n",
    "Got all that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3c78e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quick recap\n",
    "In one place, the code to implement all this looks like:\n",
    "```python\n",
    "from gravswell import quiver as qv\n",
    "from gravswell.cloudbreak import google as cb\n",
    "from google.cloud import container_v1 as container\n",
    "\n",
    "# make a repo and export\n",
    "repo = qv.ModelRepository(\"gs://ligo-quiver-demo\")\n",
    "entry = repo.add(\"my-model\", platform=qv.Platform.ONNX)\n",
    "entry.config.add_instance_group(count=4)\n",
    "export_path = entry.export_version(\n",
    "    inference_model,\n",
    "    input_shapes={\"x\": (None, INPUT_SIZE)},\n",
    "    output_names=[\"y\"]\n",
    ")\n",
    "\n",
    "# build a cluster and make it GPU friendly\n",
    "manager = cb.ClusterManager(project=\"gunny-multi-instance-dev\", zone=\"us-east4-b\")\n",
    "cluster_config = container.Cluster(\n",
    "    name=\"ligo-demo-cluster\",\n",
    "    node_pools=[container.NodePool(\n",
    "        name=\"default-pool\",\n",
    "        initial_node_count=1,\n",
    "        config=container.NodeConfig()\n",
    "    )]\n",
    ")\n",
    "cluster = manager.create_resource(cluster_config)\n",
    "cluster.deploy_gpu_drivers()\n",
    "\n",
    "# add GPU-enabled nodes to the cluster\n",
    "node_pool_config = container.NodePool(\n",
    "    name=\"tritonserver-t4-pool\",\n",
    "    initial_node_count=2,\n",
    "    config=cb.create_gpu_node_pool_config(\n",
    "        vcpus=16,\n",
    "        gpus=4,\n",
    "        gpu_type=\"t4\"\n",
    "    )\n",
    ")\n",
    "node_pool = cluster.create_resource(node_pool_config)\n",
    "\n",
    "# deploy our Triton application to these GPU nodes\n",
    "cluster.deploy(\n",
    "    \"triton.yaml\",\n",
    "    name=\"tritonserver\",\n",
    "    tag=\"20.11\",\n",
    "    bucket=\"ligo-quiver-demo\",\n",
    "    gpus=4,\n",
    "    vcpus=15\n",
    ")\n",
    "cluster.k8s_client.wait_for_deployment(\"tritonserver\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981223d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So now we're ready to make inference requests to this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf4c46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667fcc7983234b3ba10807949a24de6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tritonclient.grpc as triton\n",
    "\n",
    "# get the IP address at which the server is deployed\n",
    "ip = cluster.k8s_client.wait_for_service(\"tritonserver\")\n",
    "\n",
    "# create a client which communicates with the server\n",
    "client = triton.InferenceServerClient(f\"{ip}:8001\")\n",
    "assert client.is_server_live()\n",
    "\n",
    "# load our model into memory explicitly\n",
    "# (can also have this be done automatically\n",
    "# when the server starts)\n",
    "client.load_model(\"my-model\")\n",
    "assert client.is_model_ready(\"my-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f872b88",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's define a new inference function for making requests to the model\n",
    "- This will be subsumed by another library in the works `stillwater` for asynchronous IaaS inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c83081a3",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62747cab0cb467587e94be729824294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "def do_some_iaas_inference(model_name):\n",
    "    metadata = client.get_model_metadata(model_name).inputs[0]\n",
    "    shape = [i if i != -1 else batch_size for i in metadata.shape]\n",
    "    input = triton.InferInput(metadata.name, shape, metadata.datatype)\n",
    "\n",
    "    with utils.GpuUtilProgress() as progbar:\n",
    "        submit_task_id = progbar.add_task(\"Submitting requests\", total=N)\n",
    "        infer_task_id = progbar.add_task(\"Inferences completed\", total=N)\n",
    "\n",
    "        def callback(result, error):\n",
    "            y = result.as_numpy(\"y\")\n",
    "            progbar.update(submit_task_id, advance=len(y))\n",
    "\n",
    "        num_batches = len(dataset) // batch_size\n",
    "        for x in np.split(dataset, num_batches):\n",
    "            input.set_data_from_numpy(x)\n",
    "            client.async_infer(model_name, inputs=[input], callback=callback)\n",
    "            progbar.update(submit_task_id, advance=len(x))\n",
    "\n",
    "        while not progbar.finished:\n",
    "            time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0507b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba133b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_some_iaas_inference(\"my-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496ef55",
   "metadata": {},
   "source": [
    "## Ensemble inference\n",
    "- Adding in our \"noise removal\" model and piping its output to the input of `\"my-model\"`.\n",
    "- Start by creating a new entry for it in our model repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02a6aa98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ligo-quiver-demo/my-model/\r\n",
      "gs://ligo-quiver-demo/noise-remover/\r\n"
     ]
    }
   ],
   "source": [
    "noise_entry = repo.add(\"noise-remover\", platform=qv.Platform.ONNX)\n",
    "noise_entry.config.add_instance_group(count=4)\n",
    "noise_remover.to(\"cpu\")\n",
    "export_path = noise_entry.export_version(\n",
    "    noise_remover,\n",
    "    input_shapes={\"noisy\": (None, INPUT_SIZE)},\n",
    "    output_names=[\"cleaned\"]\n",
    ")\n",
    "! gsutil ls gs://ligo-quiver-demo/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dfbb8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ensemble inference\n",
    "- `quiver` makes piping models together straightforward\n",
    "    - `EnsembleModel` represents a meta-model that just schedules execution of `step`s consisting of other models\n",
    "        - Doesn't live on any particular GPU- schedules inference across all model instances across _all_ GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3f73a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_entry = repo.add(\"end-to-end\", platform=qv.Platform.ENSEMBLE)\n",
    "ensemble_entry.add_input(noise_entry.inputs[\"noisy\"])\n",
    "ensemble_entry.pipe(noise_entry.outputs[\"cleaned\"], entry.inputs[\"x\"])\n",
    "ensemble_entry.add_output(entry.outputs[\"y\"])\n",
    "export_path = ensemble_entry.export_version(None)\n",
    "! gsutil ls gs://ligo-quiver-demo/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77def5db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ensemble Inference\n",
    "Use the client to load the end-to-end model onto the server, then make requests to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f11479a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f7511fceba47a896eb53326c4a4e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.load_model(\"end-to-end\")\n",
    "assert client.is_model_ready(\"noise-remover\")  # ensemble loads all its \"step\" models\n",
    "assert client.is_model_ready(\"end-to-end\")\n",
    "\n",
    "do_some_iaas_inference(\"end-to-end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9cf3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-node inference\n",
    "- Kubernetes makes scaling up more server instances seamless\n",
    "- Hoping to roll this behavior into `cloudbreak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "221f6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kubernetes\n",
    "\n",
    "app_client = kubernetes.client.AppsV1Api(cluster.k8s_client._client)\n",
    "body = app_client.read_namespaced_deployment(\"tritonserver\", namespace=\"default\")\n",
    "body.spec.replicas = 2\n",
    "\n",
    "response = app_client.patch_namespaced_deployment_scale(\n",
    "    \"tritonserver\", namespace=\"default\", body=body\n",
    ")\n",
    "cluster.k8s_client.wait_for_deployment(\"tritonserver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e43690",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-node inference\n",
    "Since both nodes are exposed by the same `LoadBalancer`, same client pointing at one IP address can make requests to all 8 available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c102c799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138c9a5bc0904b9ba8032bd7119d4f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "do_some_iaas_inference(\"end-to-end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc088a18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IaaS results on LIGO data\n",
    "## Running real online and offline pipelines using Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9bef5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Challenges with streaming time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadf5e5",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- DeepClean and BBHnet produce inferences on overlapping **kernels** of data which represent a fixed length **snapshot** of the time series at one moment in time\n",
    "    - Including full kernel with _every_ request creates enormous I/O load that bottlenecks pipeline\n",
    "- Sample frames at some **inference sampling rate** $r \\leq f_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd7034",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/snapshotter_overlap.png\" height=\"auto\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4e7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Challenges with streaming time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9bad3",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Introduce a model on server which maintains the most recent snapshot as a \"state\"\n",
    "    - Connected to other models via an ensemble\n",
    "    - Only need to stream state updates of _new_ data\n",
    "    - Improves I/O, but introduces a serial step which can limit parallelism\n",
    "- Adding a snapshotter for any model soon to be introduced to `quiver`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a6b35",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<div class=\"center\">\n",
    "    <img src=\"images/snapshotter_action.png\" height=\"auto\" width=\"180px\" class=\"unpadded\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2ca37",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Experimental measurements\n",
    "- IaaS deployments fundamentally trade-off between latency, throughput, and expense\n",
    "    - Constraints of each problem define a cost surface in these variables\n",
    "    - Each point associated with a server resource usage/level of parallelism configuration\n",
    "    - Measure the variables of interest as a function of configuration\n",
    "        - Use cost function to map to cost and find optimal operating point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28deea",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<figure>\n",
    "    <img src=\"images/cost-surface.png\" height=\"auto\" width=\"800px\"/>\n",
    "    <figcaption>Surface of constant cost in \"inference space\"</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486ee1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Offline Pipelines\n",
    "- For each server, assign $k$ clients\n",
    "    - With $n$ servers, each client is assigned $\\frac{1}{nk}$ of the total dataset to process\n",
    "    - Each client has an associated snapshotter state, must be routed to same server\n",
    "- Inference sampling rate has no bearing on data generation rate\n",
    "    - Determines the _number_ of frames needed to process for a given length of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ab098",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DeepClean Offline\n",
    "- Processed ~1 month of data during O3\n",
    "<div class=\"center padded\">\n",
    "    <img src=\"images/dc-offline.png\" height=\"auto\" width=\"1200px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b37fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deepclean + BBHnet Offline\n",
    "Implemented using several different frameworks:\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/ensemble.png\" height=\"auto\" width=\"1000px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33019c6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deepclean + BBHnet Offline\n",
    "- Processed ~27 hours of data during O2\n",
    "- Strong scaling + elastic demand makes cloud ideal environment\n",
    "    - Optimal scale $\\rightarrow\\infty$\n",
    "    - Trade-off shifts to prediction quality from higher $r$ vs. expense, need to quantify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca23c5",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/e2e-offline.png\" height=\"800px\" width=\"1200px\" class=\"padded\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaa202",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HEPCloud\n",
    "Framework from HEP community for sustained, high-throughput inference using Condor APIs\n",
    "<div class=\"center\">\n",
    "    <img src=\"images/hepcloud.png\" height=\"auto\" width=\"800px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eed5f2",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DeepClean Online\n",
    "- Run on shared memory data replay frames\n",
    "- Inference sampling rate $r$ dictates _average_ data generation rate\n",
    "    - Frames become available in 1 second increments, true data generation rate is roughly a square wave\n",
    "- One data stream = one snapshotter\n",
    "    - Serial update causes queue build-up, no benefit to extra scale downstream\n",
    "    - Optimizing this step highest priority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4843e",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/dc-online.png\" height=\"auto\" width=\"1200px\" class=\"padded\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527d522",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DeepClean Online\n",
    "- Overlapping outputs due to streaming time series\n",
    "    - \"Fully online\" inference scheme causes negative impacts on PSD\n",
    "    - Can trade off some latency for improved quality through aggregation\n",
    "    - Working on improving trade-off via better training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e83409",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"images/dc-output-overlap.png\" height=\"auto\" width=\"300px\" class=\"padded\" />\n",
    "<img src=\"images/psd-latency.png\" height=\"auto\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de7c21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next Steps\n",
    "## Taking IaaS into production at LIGO"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "scroll": true,
   "slideNumber": true,
   "theme": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
